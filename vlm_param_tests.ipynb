{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1975720/3130838648.py:4: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os, sys\n",
    "import importlib\n",
    "from tqdm.autonotebook import tqdm, trange\n",
    "import pandas as pd\n",
    "import json\n",
    "import itertools\n",
    "from IPython.display import display, HTML, clear_output\n",
    "\n",
    "from FewShotTestHandler import FewShotTestHandler, optimize_hyperparameters, find_hyperparameters, test_already_stored\n",
    "from dataset import DatasetHandler\n",
    "from similarity_metrics import Similarity\n",
    "from plotting_utils import plot\n",
    "\n",
    "ENV = os.environ[\"CONDA_DEFAULT_ENV\"]\n",
    "pd.options.display.max_rows = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_dict = {}\n",
    "\n",
    "# Dataset Params - dataset.____ keys are passed into DatasetHandler constructor\n",
    "params_dict[\"dataset.name\"] = [\"smsm\", \"kinetics_100\", \"moma_act\", \"moma_sact\"]\n",
    "params_dict[\"dataset.split\"] = [\"val\"]\n",
    "params_dict[\"dataset.split_type\"] = [\"video\"]\n",
    "\n",
    "# Few-Shot Test Params - test.____ keys are passed into few-shot test call\n",
    "params_dict[\"test.n_way\"] = [None] # None gets converted into the max value for each dataset\n",
    "params_dict[\"test.n_support\"] = [0, 1, 2, 4, 8, 16]\n",
    "params_dict[\"test.n_query\"] = [None]\n",
    "params_dict[\"test.n_episodes\"] = [4]\n",
    "\n",
    "# VLM Params - vlm.____ keys are passed into VLM constructor\n",
    "if ENV == \"VLM_CLIP\":\n",
    "    from CLIP.CLIPVLM import ClipVLM as VLM\n",
    "    params_dict[\"vlm.num_frames\"] = [10]\n",
    "elif ENV == \"VLM_MILES\":\n",
    "    from MILES.wrapper import MILES_SimilarityVLM as VLM\n",
    "elif ENV == \"videoclip\":\n",
    "    from video_clip.video_clip import VideoClipVLM as VLM\n",
    "    params_dict[\"vlm.num_seconds\"] = [4]\n",
    "    params_dict[\"vlm.sample_strat\"] = [\"spread\"]\n",
    "    params_dict[\"vlm.use_cuda\"] = [True]\n",
    "elif ENV == \"VLM_UNIVL\":\n",
    "    from UNIVL.wrapper import UniVL_SimilarityVLM as VLM\n",
    "elif ENV == \"VLM_VTTWINS\":\n",
    "    from VTTWINS.wrapper import VTTWINS_SimilarityVLM as VLM\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "\n",
    "# Classifier Params - classifier.____ keys are passed into classifier constructor\n",
    "if False:\n",
    "    from classifier import WeightedTextFewShotClassifier as Classifier\n",
    "    params_dict[\"classifier.text_weight\"] = [0, 0.1, 1.0, 4.0, 10.0, 100.0]\n",
    "    #params_dict[\"classifier.metric\"] = [Similarity.COSINE, Similarity.DOT, Similarity.EUCLID]\n",
    "if True:\n",
    "    from classifier import HardPromptFewShotClassifier as Classifier\n",
    "    params_dict[\"classifier.text_weight\"] = [0, 0.1, 1.0, 2.0, 4.0, 6.0, 8.0, 10.0, 20.0, 30.0, 40.0, 50.0, 100.0]\n",
    "    params_dict[\"classifier.prompt_text\"] = [\n",
    "        \"\",\n",
    "        \"a photo showing the task of\",\n",
    "        \"an activity of\",\n",
    "        \"the video shows me\"\n",
    "    ]\n",
    "if False:\n",
    "    from classifier import NearestNeighborFewShotClassifier as Classifier\n",
    "    params_dict[\"classifier.neighbor_count\"] = [1, 2, 3, 4, 5, 10, 20]\n",
    "    params_dict[\"classifier.neighbor_weights\"] = [\"uniform\", \"distance\"]\n",
    "if False:\n",
    "    from classifier import GaussianFewShotClassifier as Classifier\n",
    "    params_dict[\"classifier.text_weight\"] = [0, 0.1, 1.0, 4.0, 10.0, 100.0]\n",
    "    params_dict[\"classifier.prior_count\"] = [0, 1, 3, 10, 30, 100]\n",
    "    params_dict[\"classifier.prior_var\"] = [0, 1, 3, 10, 30, 100]\n",
    "if False:\n",
    "    from classifier import SubVideoAverageFewShotClassifier as Classifier\n",
    "    params_dict[\"classifier.text_weight\"] = [0, 0.1, 1.0, 4.0, 10.0, 100.0]\n",
    "    params_dict[\"classifier.subvideo_segment_duration\"] = [1, 2, 5]\n",
    "    params_dict[\"classifier.subvideo_max_segments\"] = [32]\n",
    "    params_dict[\"classifier.subvideo_discard_proportion\"] = [0, 0.1, 0.25, 0.5]\n",
    "if False:\n",
    "    from classifier import TipAdapterFewShotClassifier as Classifier\n",
    "    params_dict[\"classifier.alpha\"] = [858]#[238]#[100, 120, 140] #[0.5, 1.0, 2.0]\n",
    "    params_dict[\"classifier.beta\"] = [26]#[5.07] #[2.5, 5.5, 10.0]\n",
    "    params_dict[\"classifier.finetune_lr\"] = [0]#[5.9e-4]#[1e-4, 3e-4, 1e-3, 3e-3, 1e-2]\n",
    "    params_dict[\"classifier.finetune_epochs\"] = [0]#[10] #[0, 1, 5, 10, 20]\n",
    "    params_dict[\"classifier.weight_decay\"] = [0.0001]\n",
    "if False:\n",
    "    from classifier.smsm_object_oracle import SmsmObjectOracleFewShotClassifier as Classifier\n",
    "if False:\n",
    "    from classifier.coop import CoopFewShotClassifier as Classifier\n",
    "    params_dict[\"classifier.lr\"] = [2e-4]#, 2e-3, 1e-2]\n",
    "    params_dict[\"classifier.epochs\"] = [10]\n",
    "    params_dict[\"classifier.warmup_epochs\"] = [1]\n",
    "    params_dict[\"classifier.random_augment\"] = [True]\n",
    "    params_dict[\"classifier.batch_size\"] = [8]\n",
    "if False:\n",
    "    from classifier.cona import CoNaFewShotClassifier as Classifier\n",
    "    params_dict[\"classifier.batch_size\"] = [8]\n",
    "    params_dict[\"classifier.random_augment\"] = [True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_FILENAME = \"vl_proto.csv\"\n",
    "test_handler = FewShotTestHandler(TEST_FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5407f2821e554c438e576388dca539e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1248 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PATH IS: video_clip/MMPT_updated/projects/retri/videoclip/how2.yaml\n",
      "CKPT SAVE DIR: video_clip/MMPT_updated/runs/retri/videoclip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing MMBertForEncoder: ['bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.self.value.bias', 'cls.predictions.transform.dense.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'cls.seq_relationship.weight', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.8.attention.output.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'bert.encoder.layer.6.output.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'bert.encoder.layer.11.intermediate.dense.weight', 'cls.predictions.decoder.weight', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.11.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'cls.predictions.bias', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.11.attention.self.query.weight', 'cls.predictions.transform.LayerNorm.weight', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.bias']\n",
      "- This IS expected if you are initializing MMBertForEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MMBertForEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MMBertForEncoder were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['videomlp.linear2.weight', 'videomlp.linear2.bias', 'videomlp.linear1.bias', 'videomlp.LayerNorm.bias', 'videomlp.LayerNorm.weight', 'videomlp.linear1.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df4fbb28118d4d84992b95c1b96af83d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c98ed0570de4713b049d00019df03d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3eb0bfc849e14bb7b2bdfb9a57470d3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7600 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f967931a5d6245638f201e9addc0f3ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e61ce61f5e5a4b228d8aff4c99741839",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "Weights sum to zero, can't be normalized",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/rob/vlm_benchmark/vlm_param_tests.ipynb Cell 5\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22564c4d2d42656e63686d61726b2d4750552d32227d/home/rob/vlm_benchmark/vlm_param_tests.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=52'>53</a>\u001b[0m         test_params[\u001b[39m\"\u001b[39m\u001b[39mn_way\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m support_dataset\u001b[39m.\u001b[39mcategory_count()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22564c4d2d42656e63686d61726b2d4750552d32227d/home/rob/vlm_benchmark/vlm_param_tests.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=54'>55</a>\u001b[0m     \u001b[39m# Run test\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22564c4d2d42656e63686d61726b2d4750552d32227d/home/rob/vlm_benchmark/vlm_param_tests.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=55'>56</a>\u001b[0m     test_handler\u001b[39m.\u001b[39;49mrun_few_shot_test(classifier, query_dataset, support_dataset, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mtest_params)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22564c4d2d42656e63686d61726b2d4750552d32227d/home/rob/vlm_benchmark/vlm_param_tests.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=57'>58</a>\u001b[0m clear_output()\n",
      "File \u001b[0;32m~/vlm_benchmark/FewShotTestHandler.py:78\u001b[0m, in \u001b[0;36mFewShotTestHandler.run_few_shot_test\u001b[0;34m(self, classifier, query_dataset, support_dataset, n_way, n_support, n_query, n_episodes, val_tuning_dataset)\u001b[0m\n\u001b[1;32m     75\u001b[0m dataset_iter \u001b[39m=\u001b[39m tqdm(few_shot_dataset, leave\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m     76\u001b[0m \u001b[39mfor\u001b[39;00m category_names, support_vid_paths, query_vid_paths, query_vid_labels, val_tuning_vid_paths, val_tuning_vid_labels \u001b[39min\u001b[39;00m dataset_iter:\n\u001b[0;32m---> 78\u001b[0m     query_predictions \u001b[39m=\u001b[39m classifier\u001b[39m.\u001b[39;49mpredict(category_names, support_vid_paths, query_vid_paths, val_tuning_vid_paths, val_tuning_vid_labels)\n\u001b[1;32m     80\u001b[0m     \u001b[39m# Compute accuracy for this sampled task\u001b[39;00m\n\u001b[1;32m     81\u001b[0m     correct_predictions \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msum(query_predictions \u001b[39m==\u001b[39m query_vid_labels)\n",
      "File \u001b[0;32m~/vlm_benchmark/classifier/hard_prompt.py:70\u001b[0m, in \u001b[0;36mHardPromptFewShotClassifier.predict\u001b[0;34m(self, category_names, support_video_paths, query_video_paths, val_tuning_video_paths, val_tuning_video_labels)\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprompt_location \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mend\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m     68\u001b[0m         category_names \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m.\u001b[39mstrip()\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprompt_text\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39mfor\u001b[39;00m name \u001b[39min\u001b[39;00m category_names])\n\u001b[0;32m---> 70\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mpredict(category_names, support_video_paths, query_video_paths)\n",
      "File \u001b[0;32m~/vlm_benchmark/classifier/weighted_average.py:79\u001b[0m, in \u001b[0;36mWeightedTextFewShotClassifier.predict\u001b[0;34m(self, category_names, support_video_paths, query_video_paths, val_tuning_video_paths, val_tuning_video_labels)\u001b[0m\n\u001b[1;32m     76\u001b[0m     support_embed_weights \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m [\u001b[39m1\u001b[39m] \u001b[39m*\u001b[39m n_support\n\u001b[1;32m     78\u001b[0m support_embeds \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mconcatenate(support_embeds, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m---> 79\u001b[0m prototype_embeds \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49maverage(support_embeds, axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, weights\u001b[39m=\u001b[39;49msupport_embed_weights)\n\u001b[1;32m     81\u001b[0m \u001b[39m# Compare query similarity to prototypes\u001b[39;00m\n\u001b[1;32m     82\u001b[0m query_to_proto_similarities \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmetric(query_embeds, prototype_embeds)\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36maverage\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/videoclip/lib/python3.8/site-packages/numpy/lib/function_base.py:547\u001b[0m, in \u001b[0;36maverage\u001b[0;34m(a, axis, weights, returned, keepdims)\u001b[0m\n\u001b[1;32m    545\u001b[0m     scl \u001b[39m=\u001b[39m wgt\u001b[39m.\u001b[39msum(axis\u001b[39m=\u001b[39maxis, dtype\u001b[39m=\u001b[39mresult_dtype, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkeepdims_kw)\n\u001b[1;32m    546\u001b[0m     \u001b[39mif\u001b[39;00m np\u001b[39m.\u001b[39many(scl \u001b[39m==\u001b[39m \u001b[39m0.0\u001b[39m):\n\u001b[0;32m--> 547\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mZeroDivisionError\u001b[39;00m(\n\u001b[1;32m    548\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mWeights sum to zero, can\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt be normalized\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    550\u001b[0m     avg \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmultiply(a, wgt,\n\u001b[1;32m    551\u001b[0m                       dtype\u001b[39m=\u001b[39mresult_dtype)\u001b[39m.\u001b[39msum(axis, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkeepdims_kw) \u001b[39m/\u001b[39m scl\n\u001b[1;32m    553\u001b[0m \u001b[39mif\u001b[39;00m returned:\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: Weights sum to zero, can't be normalized"
     ]
    }
   ],
   "source": [
    "vlm = None\n",
    "cur_vlm_params = None\n",
    "classifier = None\n",
    "cur_classifier_params = None\n",
    "query_dataset = None\n",
    "support_dataset = None\n",
    "cur_dataset_params = None\n",
    "\n",
    "pbar = tqdm(list(itertools.product(*params_dict.values())))\n",
    "for params in pbar:\n",
    "    # Associate keys to each param\n",
    "    params = dict(zip(params_dict.keys(), params))\n",
    "    \n",
    "    pbar.set_postfix(params)\n",
    "    \n",
    "    # vlm params\n",
    "    vlm_params = {key[4:]: val for key, val in params.items() if key.startswith(\"vlm.\")}\n",
    "    classifier_params = {key[11:]: val for key, val in params.items() if key.startswith(\"classifier.\")}\n",
    "    dataset_params = {key[8:]: val for key, val in params.items() if key.startswith(\"dataset.\")}\n",
    "    test_params = {key[5:]: val for key, val in params.items() if key.startswith(\"test.\")}\n",
    "    \n",
    "    # Update dataset\n",
    "    if query_dataset is None or cur_dataset_params != dataset_params:\n",
    "        query_dataset = DatasetHandler(**dataset_params)\n",
    "        support_dataset_params = dict(dataset_params, split=\"train\")\n",
    "        support_dataset = DatasetHandler(**support_dataset_params)\n",
    "        \n",
    "        cur_dataset_params = dataset_params\n",
    "        new_dataset = True\n",
    "    else:\n",
    "        new_dataset = False\n",
    "    \n",
    "    # Update vlm (which forces update of classifier)\n",
    "    if vlm is None or cur_vlm_params != vlm_params:\n",
    "        vlm = VLM(**vlm_params)\n",
    "        \n",
    "        cur_vlm_params = vlm_params\n",
    "        new_vlm = True\n",
    "    else:\n",
    "        new_vlm = False\n",
    "            \n",
    "    if new_vlm or classifier is None or cur_classifier_params != classifier_params:\n",
    "        classifier = Classifier(vlm, **classifier_params)\n",
    "        cur_classifier_params = classifier_params\n",
    "        \n",
    "    # Fill dataset caches\n",
    "    if new_dataset or new_vlm:\n",
    "        query_dataset.fill_cache(vlm)\n",
    "        support_dataset.fill_cache(vlm)\n",
    "        \n",
    "    # Convert n_way = None into n_way = max-ways\n",
    "    if test_params[\"n_way\"] is None:\n",
    "        test_params[\"n_way\"] = support_dataset.category_count()\n",
    "    \n",
    "    # Run test\n",
    "    test_handler.run_few_shot_test(classifier, query_dataset, support_dataset, **test_params)\n",
    "    \n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vlm_class</th>\n",
       "      <th>vlm.num_seconds</th>\n",
       "      <th>vlm.path</th>\n",
       "      <th>vlm.sample_strat</th>\n",
       "      <th>vlm.use_cuda</th>\n",
       "      <th>classifier_class</th>\n",
       "      <th>classifier.batch_size</th>\n",
       "      <th>classifier.context_len</th>\n",
       "      <th>classifier.epochs</th>\n",
       "      <th>classifier.lr</th>\n",
       "      <th>...</th>\n",
       "      <th>classifier.warmup_epochs</th>\n",
       "      <th>classifier.warmup_lr</th>\n",
       "      <th>query_dataset</th>\n",
       "      <th>support_dataset</th>\n",
       "      <th>n_way</th>\n",
       "      <th>n_support</th>\n",
       "      <th>n_query</th>\n",
       "      <th>n_episodes</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>accuracy_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>VideoClipVLM</td>\n",
       "      <td>4</td>\n",
       "      <td>video_clip/MMPT_updated/projects/retri/videocl...</td>\n",
       "      <td>spread</td>\n",
       "      <td>True</td>\n",
       "      <td>CoopFewShotClassifier</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>20</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>smsm.v.val</td>\n",
       "      <td>smsm.v.train</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>0.059318</td>\n",
       "      <td>0.004085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>VideoClipVLM</td>\n",
       "      <td>4</td>\n",
       "      <td>video_clip/MMPT_updated/projects/retri/videocl...</td>\n",
       "      <td>spread</td>\n",
       "      <td>True</td>\n",
       "      <td>CoopFewShotClassifier</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>20</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>smsm.v.val</td>\n",
       "      <td>smsm.v.train</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>0.065909</td>\n",
       "      <td>0.006818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>VideoClipVLM</td>\n",
       "      <td>4</td>\n",
       "      <td>video_clip/MMPT_updated/projects/retri/videocl...</td>\n",
       "      <td>spread</td>\n",
       "      <td>True</td>\n",
       "      <td>CoopFewShotClassifier</td>\n",
       "      <td>8</td>\n",
       "      <td>16</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>smsm.v.val</td>\n",
       "      <td>smsm.v.train</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>0.053864</td>\n",
       "      <td>0.000754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>VideoClipVLM</td>\n",
       "      <td>4</td>\n",
       "      <td>video_clip/MMPT_updated/projects/retri/videocl...</td>\n",
       "      <td>spread</td>\n",
       "      <td>True</td>\n",
       "      <td>CoopFewShotClassifier</td>\n",
       "      <td>8</td>\n",
       "      <td>16</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>smsm.v.val</td>\n",
       "      <td>smsm.v.train</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>0.055909</td>\n",
       "      <td>0.006347</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      vlm_class  vlm.num_seconds  \\\n",
       "0  VideoClipVLM                4   \n",
       "1  VideoClipVLM                4   \n",
       "2  VideoClipVLM                4   \n",
       "3  VideoClipVLM                4   \n",
       "\n",
       "                                            vlm.path vlm.sample_strat  \\\n",
       "0  video_clip/MMPT_updated/projects/retri/videocl...           spread   \n",
       "1  video_clip/MMPT_updated/projects/retri/videocl...           spread   \n",
       "2  video_clip/MMPT_updated/projects/retri/videocl...           spread   \n",
       "3  video_clip/MMPT_updated/projects/retri/videocl...           spread   \n",
       "\n",
       "   vlm.use_cuda       classifier_class  classifier.batch_size  \\\n",
       "0          True  CoopFewShotClassifier                      2   \n",
       "1          True  CoopFewShotClassifier                      2   \n",
       "2          True  CoopFewShotClassifier                      8   \n",
       "3          True  CoopFewShotClassifier                      8   \n",
       "\n",
       "   classifier.context_len  classifier.epochs  classifier.lr  ...  \\\n",
       "0                      16                 20         0.0010  ...   \n",
       "1                      16                 20         0.0010  ...   \n",
       "2                      16                 10         0.0010  ...   \n",
       "3                      16                 10         0.0002  ...   \n",
       "\n",
       "   classifier.warmup_epochs  classifier.warmup_lr  query_dataset  \\\n",
       "0                       1.0               0.00001     smsm.v.val   \n",
       "1                       1.0               0.00001     smsm.v.val   \n",
       "2                       1.0               0.00001     smsm.v.val   \n",
       "3                       1.0               0.00001     smsm.v.val   \n",
       "\n",
       "  support_dataset n_way  n_support  n_query  n_episodes  accuracy  \\\n",
       "0    smsm.v.train   100          1      NaN           4  0.059318   \n",
       "1    smsm.v.train   100          1      NaN           4  0.065909   \n",
       "2    smsm.v.train   100          1      NaN           4  0.053864   \n",
       "3    smsm.v.train   100          1      NaN           4  0.055909   \n",
       "\n",
       "   accuracy_std  \n",
       "0      0.004085  \n",
       "1      0.006818  \n",
       "2      0.000754  \n",
       "3      0.006347  \n",
       "\n",
       "[4 rows x 21 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(test_handler.results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Best Hyperparameters on Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vlm_class</th>\n",
       "      <th>classifier_class</th>\n",
       "      <th>query_dataset</th>\n",
       "      <th>support_dataset</th>\n",
       "      <th>n_way</th>\n",
       "      <th>n_support</th>\n",
       "      <th>n_query</th>\n",
       "      <th>n_episodes</th>\n",
       "      <th>vlm.num_seconds</th>\n",
       "      <th>vlm.path</th>\n",
       "      <th>vlm.sample_strat</th>\n",
       "      <th>vlm.use_cuda</th>\n",
       "      <th>classifier.batch_size</th>\n",
       "      <th>classifier.context_len</th>\n",
       "      <th>classifier.epochs</th>\n",
       "      <th>classifier.lr</th>\n",
       "      <th>classifier.random_augment</th>\n",
       "      <th>classifier.warmup_epochs</th>\n",
       "      <th>classifier.warmup_lr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>VideoClipVLM</td>\n",
       "      <td>CoopFewShotClassifier</td>\n",
       "      <td>smsm.v.val</td>\n",
       "      <td>smsm.v.train</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>video_clip/MMPT_updated/projects/retri/videocl...</td>\n",
       "      <td>spread</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>20</td>\n",
       "      <td>0.001</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      vlm_class       classifier_class query_dataset support_dataset  n_way  \\\n",
       "0  VideoClipVLM  CoopFewShotClassifier    smsm.v.val    smsm.v.train    100   \n",
       "\n",
       "   n_support  n_query  n_episodes  vlm.num_seconds  \\\n",
       "0          1      NaN           4                4   \n",
       "\n",
       "                                            vlm.path vlm.sample_strat  \\\n",
       "0  video_clip/MMPT_updated/projects/retri/videocl...           spread   \n",
       "\n",
       "   vlm.use_cuda  classifier.batch_size  classifier.context_len  \\\n",
       "0          True                      2                      16   \n",
       "\n",
       "   classifier.epochs  classifier.lr  classifier.random_augment  \\\n",
       "0                 20          0.001                      False   \n",
       "\n",
       "   classifier.warmup_epochs  classifier.warmup_lr  \n",
       "0                       1.0               0.00001  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_hyperparam_values = find_hyperparameters(\n",
    "    test_handler.results,\n",
    "    hyperparam_cols=[col for col in test_handler.results if col.startswith(\"classifier.\") or col.startswith(\"vlm.\")]\n",
    ")\n",
    "display(best_hyperparam_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3146a40cf13a4c8bbed1ec82887bc872",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PATH IS: video_clip/MMPT_updated/projects/retri/videoclip/how2.yaml\n",
      "CKPT SAVE DIR: video_clip/MMPT_updated/runs/retri/videoclip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing MMBertForEncoder: ['bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.11.attention.self.value.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.output.dense.bias', 'cls.predictions.transform.dense.weight', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.11.output.LayerNorm.bias', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.query.weight', 'cls.seq_relationship.bias', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.11.attention.output.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.8.output.LayerNorm.bias', 'cls.predictions.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.10.intermediate.dense.bias']\n",
      "- This IS expected if you are initializing MMBertForEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MMBertForEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MMBertForEncoder were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['videomlp.LayerNorm.bias', 'videomlp.linear1.bias', 'videomlp.linear2.bias', 'videomlp.linear1.weight', 'videomlp.linear2.weight', 'videomlp.LayerNorm.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e83bd2d9fd654e28bf66de68518e8a82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8384a2c912b44793a96495d8b64b4819",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ce5b0f0cc9c42b784a94718d8b5e653",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7600 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b22b2657b2a840b3a52fd7f99e8de4ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c56ce9202f8342a699345353e65a7cd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rob/miniconda3/envs/videoclip/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch     0: Acc = 0.060, Loss = 4.476\n",
      "Epoch     1: Acc = 0.070, Loss = 4.388\n",
      "Epoch     2: Acc = 0.080, Loss = 4.201\n",
      "Epoch     3: Acc = 0.110, Loss = 4.002\n",
      "Epoch     4: Acc = 0.130, Loss = 3.825\n",
      "Epoch     5: Acc = 0.130, Loss = 3.666\n",
      "Epoch     6: Acc = 0.190, Loss = 3.438\n",
      "Epoch     7: Acc = 0.240, Loss = 3.254\n",
      "Epoch     8: Acc = 0.310, Loss = 2.912\n",
      "Epoch     9: Acc = 0.410, Loss = 2.590\n",
      "Epoch    10: Acc = 0.540, Loss = 2.216\n",
      "Epoch    11: Acc = 0.690, Loss = 1.766\n",
      "Epoch    12: Acc = 0.820, Loss = 1.477\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/rob/vlm_benchmark/vlm_param_tests.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22564c4d2d42656e63686d61726b2d4750552d32227d/home/rob/vlm_benchmark/vlm_param_tests.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=90'>91</a>\u001b[0m         support_dataset\u001b[39m.\u001b[39mfill_cache(vlm)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22564c4d2d42656e63686d61726b2d4750552d32227d/home/rob/vlm_benchmark/vlm_param_tests.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=92'>93</a>\u001b[0m     \u001b[39m# Run test\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22564c4d2d42656e63686d61726b2d4750552d32227d/home/rob/vlm_benchmark/vlm_param_tests.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=93'>94</a>\u001b[0m     final_test_handler\u001b[39m.\u001b[39;49mrun_few_shot_test(classifier, query_dataset, support_dataset, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mtest_params)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22564c4d2d42656e63686d61726b2d4750552d32227d/home/rob/vlm_benchmark/vlm_param_tests.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=95'>96</a>\u001b[0m clear_output()\n",
      "File \u001b[0;32m~/vlm_benchmark/FewShotTestHandler.py:76\u001b[0m, in \u001b[0;36mFewShotTestHandler.run_few_shot_test\u001b[0;34m(self, classifier, query_dataset, support_dataset, n_way, n_support, n_query, n_episodes)\u001b[0m\n\u001b[1;32m     73\u001b[0m dataset_iter \u001b[39m=\u001b[39m tqdm(few_shot_dataset, leave\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m     74\u001b[0m \u001b[39mfor\u001b[39;00m category_names, support_vid_paths, query_vid_paths, query_vid_labels \u001b[39min\u001b[39;00m dataset_iter:\n\u001b[0;32m---> 76\u001b[0m     query_predictions \u001b[39m=\u001b[39m classifier\u001b[39m.\u001b[39;49mpredict(category_names, support_vid_paths, query_vid_paths)\n\u001b[1;32m     78\u001b[0m     \u001b[39m# Compute accuracy for this sampled task\u001b[39;00m\n\u001b[1;32m     79\u001b[0m     correct_predictions \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msum(query_predictions \u001b[39m==\u001b[39m query_vid_labels)\n",
      "File \u001b[0;32m~/vlm_benchmark/classifier/coop.py:131\u001b[0m, in \u001b[0;36mCoopFewShotClassifier.predict\u001b[0;34m(self, category_names, support_video_paths, query_video_paths)\u001b[0m\n\u001b[1;32m    128\u001b[0m logits \u001b[39m=\u001b[39m coop_module(vid_embeds)\n\u001b[1;32m    129\u001b[0m loss \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mcross_entropy(logits, vid_labels)\n\u001b[0;32m--> 131\u001b[0m total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39;49mitem() \u001b[39m*\u001b[39m \u001b[39mlen\u001b[39m(vid_paths)\n\u001b[1;32m    132\u001b[0m total_correct \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (logits\u001b[39m.\u001b[39margmax(dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m==\u001b[39m vid_labels)\u001b[39m.\u001b[39msum()\n\u001b[1;32m    133\u001b[0m total_count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(vid_paths)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Change params_dict to only include dataset and test info, then run tests with best hyperparameter values\n",
    "test_split_params_dict = {\n",
    "    \"dataset.split\": [\"test\"]\n",
    "}\n",
    "for key, val in params_dict.items():\n",
    "    if key.startswith(\"classifier.\") or key.startswith(\"vlm.\") or key == \"dataset.split\":\n",
    "        continue\n",
    "    test_split_params_dict[key] = val\n",
    "    \n",
    "final_test_handler = FewShotTestHandler(f\"test.{TEST_FILENAME}\")\n",
    "    \n",
    "vlm = None\n",
    "cur_vlm_params = None\n",
    "classifier = None\n",
    "cur_classifier_params = None\n",
    "query_dataset = None\n",
    "support_dataset = None\n",
    "cur_dataset_params = None\n",
    "\n",
    "pbar = tqdm(list(itertools.product(*test_split_params_dict.values())))\n",
    "for params in pbar:\n",
    "    # Associate keys to each param\n",
    "    params = dict(zip(test_split_params_dict.keys(), params))\n",
    "    \n",
    "    # Determine dataset and test parameters\n",
    "    dataset_params = {key[8:]: val for key, val in params.items() if key.startswith(\"dataset.\")}\n",
    "    test_params = {key[5:]: val for key, val in params.items() if key.startswith(\"test.\")}\n",
    "    \n",
    "    # Update dataset\n",
    "    if query_dataset is None or cur_dataset_params != dataset_params:\n",
    "        query_dataset = DatasetHandler(**dataset_params)\n",
    "        support_dataset_params = dict(dataset_params, split=\"train\")\n",
    "        support_dataset = DatasetHandler(**support_dataset_params)\n",
    "        \n",
    "        # Construct dummy val dataset to get id for filtering dataframe results with corresponding hyperparameters\n",
    "        val_dataset = DatasetHandler(**dict(dataset_params, split=\"val\"))\n",
    "        \n",
    "        cur_dataset_params = dataset_params\n",
    "        new_dataset = True\n",
    "    else:\n",
    "        new_dataset = False\n",
    "        \n",
    "    # Determine vlm and classifier params from hyperparameter dataframe\n",
    "    matched_hyperparam_values = np.ones(len(best_hyperparam_values)).astype(bool)\n",
    "    matched_hyperparam_values &= (best_hyperparam_values[\"vlm_class\"] == VLM.__name__) & (best_hyperparam_values[\"classifier_class\"] == Classifier.__name__)\n",
    "    matched_hyperparam_values &= (best_hyperparam_values[\"query_dataset\"] == val_dataset.id()) & (best_hyperparam_values[\"support_dataset\"] == support_dataset.id())\n",
    "    for col, val in test_params.items():\n",
    "        if pd.isna(val):\n",
    "            matched_hyperparam_values &= pd.isna(best_hyperparam_values[col])        \n",
    "        else:\n",
    "            matched_hyperparam_values &= (best_hyperparam_values[col] == val)\n",
    "    matched_hyperparam_values = best_hyperparam_values[matched_hyperparam_values].reset_index(drop=True)\n",
    "    \n",
    "    vlm_params = {}\n",
    "    classifier_params = {}\n",
    "    for col in matched_hyperparam_values.columns:\n",
    "        if col.startswith(\"vlm.\"):\n",
    "            val = matched_hyperparam_values.loc[0, col]\n",
    "            if not pd.isna(val):\n",
    "                vlm_params[col[4:]] = val\n",
    "        \n",
    "        if col.startswith(\"classifier.\"):\n",
    "            val = matched_hyperparam_values.loc[0, col]\n",
    "            if not pd.isna(val):\n",
    "                classifier_params[col[11:]] = val\n",
    "        \n",
    "    for key, val in vlm_params.items():\n",
    "        params[f\"vlm.{key}\"] = val\n",
    "    for key, val in classifier_params.items():\n",
    "        params[f\"classifier.{key}\"] = val\n",
    "    pbar.set_postfix(params)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Update vlm (which forces update of classifier)\n",
    "    if vlm is None or cur_vlm_params != vlm_params:\n",
    "        vlm = VLM(**vlm_params)\n",
    "        \n",
    "        cur_vlm_params = vlm_params\n",
    "        new_vlm = True\n",
    "    else:\n",
    "        new_vlm = False\n",
    "            \n",
    "    if new_vlm or classifier is None or cur_classifier_params != classifier_params:\n",
    "        classifier = Classifier(vlm, **classifier_params)\n",
    "        cur_classifier_params = classifier_params\n",
    "        \n",
    "    # Fill dataset caches\n",
    "    if new_dataset or new_vlm:\n",
    "        query_dataset.fill_cache(vlm)\n",
    "        support_dataset.fill_cache(vlm)\n",
    "        \n",
    "    # Convert n_way = None into n_way = max-ways\n",
    "    if test_params[\"n_way\"] is None:\n",
    "        test_params[\"n_way\"] = support_dataset.category_count()\n",
    "    \n",
    "    # Run test\n",
    "    final_test_handler.run_few_shot_test(classifier, query_dataset, support_dataset, **test_params)\n",
    "    \n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(final_test_handler.results.sort_values([\"vlm_class\", \"classifier_class\", \"query_dataset\", \"n_support\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(\n",
    "    final_test_handler,\n",
    "    x_col=\"n_support\",\n",
    "    y_col=\"accuracy\",\n",
    "    plot_descriptor_cols=[\"dataset\", \"classifier_class\"],\n",
    "    line_descriptor_cols=[\"vlm_class\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('videoclip')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "576597d045da71b18680487735a015f28433d9aa438b3c061008c825d6c37722"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
